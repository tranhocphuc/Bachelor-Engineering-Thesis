{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUQxSIiBtPhX"
   },
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 146345,
     "status": "ok",
     "timestamp": 1603142412855,
     "user": {
      "displayName": "Christian Richardson Escobia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjdPT7lC12Lnkl07FaCCF-69aiWOLTRNP49Dmh7=s64",
      "userId": "09982762860506727889"
     },
     "user_tz": -120
    },
    "id": "fgmr4ojttATx",
    "outputId": "a55e2f14-f8ed-425e-95f3-853b3d0c2e7d"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 722,
     "status": "ok",
     "timestamp": 1603142430475,
     "user": {
      "displayName": "Christian Richardson Escobia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjdPT7lC12Lnkl07FaCCF-69aiWOLTRNP49Dmh7=s64",
      "userId": "09982762860506727889"
     },
     "user_tz": -120
    },
    "id": "xvPZo4oStStl",
    "outputId": "62899ae6-e4cb-4306-c6bc-fb2ecacf205d"
   },
   "outputs": [],
   "source": [
    "%cd /gdrive/My Drive/phuc_code_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1JlNYx9-tS-6"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from google.colab import widgets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from utils import file_helper, feature_extraction\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7jzeibwfZ1bI"
   },
   "outputs": [],
   "source": [
    "%ls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0piIWXwDtqXS"
   },
   "source": [
    "# Construct Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o4PoiCD3tnRT"
   },
   "outputs": [],
   "source": [
    "from scipy.signal import butter, lfilter, freqz\n",
    "from scipy import signal \n",
    "from utils.peaks_util import get_echo_peaks\n",
    "\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, frequency = \"1.14MHz\"):\n",
    "        self.config = file_helper.get_config()\n",
    "        self.options = file_helper.get_config()['options']\n",
    "#         self.selected_frequency = list(self.options.keys())[0]\n",
    "        self.selected_frequency = frequency\n",
    "        self.selected_element = self.options[self.selected_frequency]\n",
    "        self.LOW_PASS = self.config['low']\n",
    "        self.HIGH_PASS = self.config['high']\n",
    "\n",
    "        if 'low' in self.selected_element:\n",
    "            self.LOW_PASS = self.selected_element['low']\n",
    "        if 'high' in self.selected_element:\n",
    "            self.HIGH_PASS = self.selected_element['high']\n",
    "            \n",
    "        self.NOISE_SIZE = self.selected_element['noise_size']\n",
    "        self.DATA_HEADERS_SIZE = self.config['raw_data_header']\n",
    "        self.ECHO_SIZE_LEFT = self.selected_element['echo_size_left']\n",
    "        self.ECHO_SIZE = self.selected_element['echo_size']\n",
    "        self.order = self.config['order']\n",
    "        self.VARIANCE_THRESHOLD = self.selected_element['variance_threshold']\n",
    "\n",
    "\n",
    "    def butter_lowpass(self, cutoff, fs, order):\n",
    "        nyq = 0.5 * fs\n",
    "        normal_cutoff = cutoff / nyq\n",
    "        b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "        return b, a\n",
    "\n",
    "    def butter_highpass(self, cutoff, fs, order):\n",
    "        nyq = 0.5 * fs\n",
    "        normal_cutoff = cutoff / nyq\n",
    "        b, a = butter(order, normal_cutoff, btype='high', analog=False)\n",
    "        return b, a\n",
    "\n",
    "    def apply_filter(self, data):\n",
    "        b, a = self.butter_lowpass(\n",
    "            self.LOW_PASS, self.selected_element['value'], order=self.order)\n",
    "        y = signal.filtfilt(b, a, data)\n",
    "\n",
    "        b, a = self.butter_highpass(\n",
    "            self.HIGH_PASS, self.selected_element['value'], self.order)\n",
    "        y_ = signal.filtfilt(b, a, y)\n",
    "        return y_\n",
    "\n",
    "    # Step 1 and 2\n",
    "    # This method removes offset from time domain data\n",
    "    # @params filename\n",
    "    # @returns dataframe\n",
    "\n",
    "    def get_time_domain_without_offset(self, filename):\n",
    "        data_frame = pd.read_csv(filename, skiprows=[0], header=None)\n",
    "        required_data_frame = data_frame.iloc[:, self.DATA_HEADERS_SIZE:]\n",
    "        return required_data_frame.sub(required_data_frame.mean(axis=1), axis=0)\n",
    "\n",
    "    # Step 3\n",
    "    # this method applies low and high pass filter to time domain data\n",
    "    # @params data_frame\n",
    "    # @returns list\n",
    "\n",
    "    def get_filtered_values(self, data_frame):\n",
    "        new_data = []\n",
    "        for data in data_frame.values:\n",
    "            new_data.append(data)\n",
    "        new_data = np.array(new_data)\n",
    "        return new_data\n",
    "\n",
    "    def get_echo_set_location(self, data):\n",
    "        data = np.array(data)\n",
    "        no_of_windows = round(data.size[1]/self.ECHO_SIZE)\n",
    "        echo_set = []\n",
    "        for d in data:\n",
    "            window_peak_locations = []\n",
    "            for i in range(0, no_of_windows):\n",
    "                window_peak = d[i*self.ECHO_SIZE:(i+1)*self.ECHO_SIZE].max()\n",
    "                if window_peak >= self.THRESHOLD:\n",
    "                    window_peak_location = i*self.ECHO_SIZE + \\\n",
    "                        d[i*self.ECHO_SIZE:(i+1)*self.ECHO_SIZE].argmax()\n",
    "                    if i > 0 or window_peak_location > self.ECHO_SIZE_LEFT:\n",
    "                        window_peak_locations.append(window_peak_location)\n",
    "            echos = []\n",
    "            if len(window_peak_locations):\n",
    "                echos = [window_peak_locations[0]]\n",
    "                prev_echo = window_peak_locations[0]\n",
    "                for w in window_peak_locations:\n",
    "                    if prev_echo - w > self.ECHO_SIZE:\n",
    "                        echos.append(w)\n",
    "                    prev_echo = w\n",
    "            echo_set.append(echos)\n",
    "        return echo_set\n",
    "\n",
    "    def get_echo_with_index(self, data_values, rolling_variance_dataframe_value=[]):\n",
    "        echo_list = []\n",
    "        if not len(rolling_variance_dataframe_value):\n",
    "            data_values = data_values[self.NOISE_SIZE:]\n",
    "            dfObj = pd.DataFrame([data_values])\n",
    "\n",
    "            rolling_variance_dataframe = dfObj.rolling(\n",
    "                window=100, axis=1).var()\n",
    "            peaks, _ = signal.find_peaks(\n",
    "                rolling_variance_dataframe.values[0], height=self.VARIANCE_THRESHOLD)\n",
    "        else:\n",
    "            peaks, _ = signal.find_peaks(\n",
    "                rolling_variance_dataframe_value, height=self.VARIANCE_THRESHOLD)\n",
    "        if len(peaks):\n",
    "            echo_peaks = get_echo_peaks(\n",
    "                peaks, self.ECHO_SIZE, self.ECHO_SIZE_LEFT)\n",
    "            if len(echo_peaks):\n",
    "                for index, e in enumerate(echo_peaks):\n",
    "                    echo_left_size = e - self.ECHO_SIZE_LEFT\n",
    "                    echo_right_size = echo_left_size + self.ECHO_SIZE\n",
    "                    if echo_right_size > len(data_values):\n",
    "                        continue\n",
    "                    echo_data = {\n",
    "                        #Take the entire waveform\n",
    "                        'ECHO': data_values,\n",
    "                    }\n",
    "                    echo_list.append(echo_data)\n",
    "            return echo_list\n",
    "        return None\n",
    "\n",
    "    def find_echos(self, data_values):\n",
    "        data_values = data_values[:, self.NOISE_SIZE:]\n",
    "        dfObj = pd.DataFrame(data_values)\n",
    "        rolling_variance_dataframe = dfObj.rolling(\n",
    "            window=100, axis=1).var()\n",
    "        echo_list = []\n",
    "        for i, d in enumerate(rolling_variance_dataframe.values):\n",
    "            echo_data = self.get_echo_with_index(\n",
    "                data_values[i], d)\n",
    "            if echo_data:\n",
    "                echo_list = echo_list + echo_data\n",
    "        if len(echo_list):\n",
    "            df = pd.DataFrame.from_dict(echo_list)\n",
    "            df_with_echo = pd.DataFrame(df.ECHO.tolist()) \n",
    "            return df_with_echo\n",
    "        return None\n",
    "    def get_features_from_echo(self, echos_data, row):\n",
    "        df_fft = echos_data.iloc[:, 1:]\n",
    "        fft_list = feature_extraction.fft_from_data_frame(\n",
    "            df_fft, self.selected_element['value'], \n",
    "            self.config['low'], \n",
    "            self.config['high'])\n",
    "        fft_set = pd.DataFrame(fft_list)\n",
    "        fft_set['type'] = row['type']\n",
    "        fft_set = fft_set.set_index(\n",
    "            ['type']).reset_index()\n",
    "\n",
    "        return fft_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLLL7rxKtwUw"
   },
   "source": [
    "# Data Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLQjWMzXtabV"
   },
   "outputs": [],
   "source": [
    "def load_file(directory):\n",
    "    try:\n",
    "        directory = directory\n",
    "        files = file_helper.files_from_directory(directory)\n",
    "        return files\n",
    "    except:\n",
    "        raise Exception('Please choose a correct file!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ITLYe8RtyiO"
   },
   "outputs": [],
   "source": [
    "def save_echo_to_file(directory, freq = '1.14MHz'):\n",
    "    files = load_file(directory)\n",
    "    print(files)\n",
    "    processor = DataProcessor(frequency = freq)\n",
    "    echo_data_set = pd.DataFrame()\n",
    "    fft_data_set = pd.DataFrame()\n",
    "    begin = time.time()\n",
    "    for index, file in enumerate(files):\n",
    "        print(\"Index: {}\".format(index))\n",
    "        if len(file['absolute_path'].split('/')) >= 4:\n",
    "            try: \n",
    "                time_domain_data_set = processor.get_time_domain_without_offset(\n",
    "                    file['absolute_path'])\n",
    "\n",
    "            except: \n",
    "                continue\n",
    "            start = time.time()\n",
    "            filtered_data_values = processor.get_filtered_values(\n",
    "                time_domain_data_set)\n",
    "            echos_data = processor.find_echos(\n",
    "                filtered_data_values)\n",
    "            end = time.time()\n",
    "            print(\"Process time: {}\".format(end - start))\n",
    "            if isinstance(echos_data, pd.DataFrame):\n",
    "                row = {\n",
    "                    'type': file['absolute_path'].split('/')[-4].upper(),\n",
    "                }\n",
    "                fft_data_set = fft_data_set.append(\n",
    "                    processor.get_features_from_echo(echos_data, row), ignore_index=True)\n",
    "                echos_data['type'] = row['type']\n",
    "                echos_data = echos_data.set_index(\n",
    "                    ['type']).reset_index()\n",
    "                echo_data_set = echo_data_set.append(\n",
    "                    echos_data, ignore_index=True)\n",
    "    final = time.time()\n",
    "    print(\"Total run time: {}\".format(final - begin))      \n",
    "    echo_data_set = echo_data_set.dropna()\n",
    "    fft_data_set = fft_data_set.dropna()\n",
    "    return echo_data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOvGSbogLr12"
   },
   "source": [
    "# Export Echo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2271956,
     "status": "ok",
     "timestamp": 1600719476100,
     "user": {
      "displayName": "Christian Richardson Escobia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjdPT7lC12Lnkl07FaCCF-69aiWOLTRNP49Dmh7=s64",
      "userId": "09982762860506727889"
     },
     "user_tz": -120
    },
    "id": "5RFUvFCPt1GU",
    "outputId": "1b7c7374-c3bf-441f-d3ee-94c64a291830"
   },
   "outputs": [],
   "source": [
    "echo_dataset= save_echo_to_file(directory = './moving_class/114MHz', freq='1.14MHz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "executionInfo": {
     "elapsed": 2270195,
     "status": "ok",
     "timestamp": 1600719476733,
     "user": {
      "displayName": "Christian Richardson Escobia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjdPT7lC12Lnkl07FaCCF-69aiWOLTRNP49Dmh7=s64",
      "userId": "09982762860506727889"
     },
     "user_tz": -120
    },
    "id": "L1efTUd9GY44",
    "outputId": "fd91df61-785d-4653-c725-7c309589cbc7"
   },
   "outputs": [],
   "source": [
    "echo_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCEeeiFkuAc3"
   },
   "source": [
    "# Save Datasets into CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7O0Cgeant_44"
   },
   "outputs": [],
   "source": [
    "# echo_dataset.to_csv(r'full_echo.csv', index = False,header = True)\n",
    "\n",
    "# fft_dataset.to_csv(r'updated_fft_dataset_merged_newestfreq.csv',index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iGm20JCUzMEg"
   },
   "outputs": [],
   "source": [
    "# echo_dataset = pd.read_csv('full_echo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "executionInfo": {
     "elapsed": 2268839,
     "status": "ok",
     "timestamp": 1600719479800,
     "user": {
      "displayName": "Christian Richardson Escobia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjdPT7lC12Lnkl07FaCCF-69aiWOLTRNP49Dmh7=s64",
      "userId": "09982762860506727889"
     },
     "user_tz": -120
    },
    "id": "lidu5sYzjXEb",
    "outputId": "6586e64c-329d-4414-9ee7-e1d2186c2b57"
   },
   "outputs": [],
   "source": [
    "calculate_instance(echo_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJpTbRuBWGwe"
   },
   "source": [
    "Calculating Instance:\n",
    "\n",
    "HUMAN :  47607 training instances\n",
    "\n",
    "BICYCLE :  11057 training instances\n",
    "\n",
    "WALL :  12974 training instances\n",
    "\n",
    "PILLAR :  16832 training instances\n",
    "\n",
    "CAR :  28367 training instances\n",
    "\n",
    "Non Human:  69230 training instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6Eu2X1O-rLY"
   },
   "source": [
    "# Binary Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VsAYoJ-5ocpF"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "def process_data(line):\n",
    "    label, feature = line.iloc[:,0],line.iloc[:,1:]\n",
    "    label = label.replace(['BICYCLE','WALL','PILLAR','CAR'], 'NON_HUMAN')\n",
    "    label = label.replace('NON_HUMAN', 0)\n",
    "    label = label.replace('HUMAN', 1)\n",
    "    feature_norm = preprocessing.normalize(feature)\n",
    "    feature_X = feature_norm.reshape(-1,1,5704)\n",
    "    return label, feature_X\n",
    "\n",
    "def split_dataset(dataframe, method = True): \n",
    "    if method == True: \n",
    "        # Use pandas copy method to avioid any modifications to the data or indices of the copy will not be\n",
    "        # reflected in the original object. \n",
    "        labels = dataframe.iloc[:,0].copy() \n",
    "        features = dataframe.iloc[:,1:].copy()\n",
    "    # else: \n",
    "    #     labels = fft.iloc[:,0].copy()\n",
    "    #     features = fft.iloc[:,1:].copy()\n",
    "    return labels, features\n",
    "\n",
    "def calculate_instance(data): \n",
    "    print(\"Calculating Instance:\")\n",
    "    for i in ['HUMAN','BICYCLE','WALL','PILLAR','CAR']:\n",
    "        name = data.loc[data['type'] == i]\n",
    "        print(i, \": \", name.shape[0], \"training instances\")\n",
    "    non_name = data.loc[data['type'] != 'HUMAN']\n",
    "    print(\"Non Human: \", non_name.shape[0], \"training instances\")\n",
    "\n",
    "def calculate_valid_size (test_size, train_size): \n",
    "    train_valid_size = 1.0 - test_size\n",
    "    valid_size = np.around((test_size*1.0)/train_valid_size, 2)\n",
    "    return valid_size\n",
    "\n",
    "# Train/Test/Valid split function\n",
    "def ttv_split(features, labels, size_test, size_valid):\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(features,labels, test_size = size_test, shuffle = True, \n",
    "                                                                random_state = 42, stratify = labels)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size = size_valid , shuffle = True, \n",
    "                                                      random_state = 42, stratify = y_train_val)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eK7Hergc-qOl"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "chunksize = 1000\n",
    "label, feature = None, None\n",
    "for chunk in pd.read_csv(\"full_echo.csv\",chunksize=chunksize, skiprows = 0):\n",
    "    label_en, feature_X = process_data(chunk)\n",
    "    if label is None and feature is None : \n",
    "        label = label_en\n",
    "        feature = feature_X\n",
    "    else:\n",
    "        label = np.append(label,label_en, axis = 0)\n",
    "        feature = np.append(feature, feature_X, axis = 0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 858,
     "status": "ok",
     "timestamp": 1600721788084,
     "user": {
      "displayName": "Christian Richardson Escobia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjdPT7lC12Lnkl07FaCCF-69aiWOLTRNP49Dmh7=s64",
      "userId": "09982762860506727889"
     },
     "user_tz": -120
    },
    "id": "DWjsaXH9aNMw",
    "outputId": "8d28ec8e-38d2-437a-9e4b-0a2a5507439a"
   },
   "outputs": [],
   "source": [
    "# len(val_y[val_y == 1])/len(val_y)\n",
    "print(len(label[label == 1]))\n",
    "print(len(label[label == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yAkPPoNoonoc"
   },
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(feature,label, test_size = 0.1, shuffle = True, \n",
    "                                                                random_state = 42, stratify = label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aURJmCfxA3Om"
   },
   "outputs": [],
   "source": [
    "# from numpy import save \n",
    "# save('X_train_val.npy', X_train_val)\n",
    "# save('y_train_val.npy', y_train_val)\n",
    "# save('X_test.npy', X_test)\n",
    "# save('y_test.npy',y_test)\n",
    "train_val_X = np.load('X_train_val.npy')\n",
    "train_val_y = np.load('y_train_val.npy')\n",
    "# test_X = np.load('Dataset/split_data/Binary Split/X_test.npy')\n",
    "# test_y = np.load('Dataset/split_data/Binary Split/y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u7sbKKSNBpZ0"
   },
   "outputs": [],
   "source": [
    " X_train, X_val, y_train, y_val = train_test_split(train_val_X, train_val_y, test_size = 0.11 , shuffle = True, \n",
    "                                                      random_state = 42, stratify = train_val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FypXhiiaESUg"
   },
   "outputs": [],
   "source": [
    "from numpy import save \n",
    "save('X_train.npy', X_train)\n",
    "save('y_train.npy', y_train)\n",
    "save('X_val.npy', X_val)\n",
    "save('y_val.npy',y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kgwqXA_GFBGP"
   },
   "source": [
    "# Multiclass Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vof6uBldPMBv"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "def process_data(line):\n",
    "    label, feature = line.iloc[:,0],line.iloc[:,1:]\n",
    "    label = np.where(label =='HUMAN',0, label)\n",
    "    label = np.where(label == 'BICYCLE',1,label)\n",
    "    label = np.where(label == 'PILLAR',2,label)\n",
    "    label = np.where(label == 'WALL',3,label)\n",
    "    label = np.where(label == 'CAR',4,label)\n",
    "    label_en = tf.keras.utils.to_categorical(label, num_classes = 5)\n",
    "    feature_norm = preprocessing.normalize(feature)\n",
    "    feature_X = feature_norm.reshape(-1,1,5704)\n",
    "    return label_en, feature_X\n",
    "\n",
    "def print_label_en(label):\n",
    "    print(\"Human: \",len(label[label == 0]))\n",
    "    print(\"Bicycle: \", len(label[label == 1]))\n",
    "    print(\"Wall: \", len(label[label == 2]))\n",
    "    print(\"Pillar: \", len(label[label == 3]))\n",
    "    print(\"Car: \", len(label[label == 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 311429,
     "status": "error",
     "timestamp": 1600886169033,
     "user": {
      "displayName": "Christian Richardson Escobia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjdPT7lC12Lnkl07FaCCF-69aiWOLTRNP49Dmh7=s64",
      "userId": "09982762860506727889"
     },
     "user_tz": -120
    },
    "id": "1OilQhnmrAOk",
    "outputId": "20b64173-122a-480d-e6b9-8709347cfe6c"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import time \n",
    "chunksize = 1000\n",
    "label, feature = None, None\n",
    "count = 0\n",
    "\n",
    "for chunk in pd.read_csv(\"full_echo.csv\",engine = 'python', chunksize=chunksize, skiprows = 0):\n",
    "    label_en, feature_X = process_data(chunk)\n",
    "    count += 1 \n",
    "    print(count,\"\\n\",label_en, label_en.shape)\n",
    "    if label is None and feature is None : \n",
    "        label = label_en\n",
    "        feature = feature_X\n",
    "    else:\n",
    "        label = np.append(label,label_en, axis = 0)\n",
    "        feature = np.append(feature, feature_X, axis = 0)\n",
    "    stop = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 739,
     "status": "ok",
     "timestamp": 1600886188426,
     "user": {
      "displayName": "Christian Richardson Escobia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjdPT7lC12Lnkl07FaCCF-69aiWOLTRNP49Dmh7=s64",
      "userId": "09982762860506727889"
     },
     "user_tz": -120
    },
    "id": "O9iodAserFvF",
    "outputId": "8e897812-27a5-4d3f-ecb5-563bb528e349"
   },
   "outputs": [],
   "source": [
    "count_label = tf.math.reduce_sum(label, axis = 0)\n",
    "print(count_label) \n",
    "#HUMAN, BICYCLE, PILLAR, WALL, CAR\n",
    "#0    ,    1   ,    2  ,  3  ,  4\n",
    "#Desired Output:\n",
    "#47607,11057, 16832, 12974, 28367"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LwX5QMGirvHp"
   },
   "outputs": [],
   "source": [
    "#Train_Val/Test split\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(feature,label, test_size = 0.1, shuffle = True, \n",
    "                                                                random_state = 42, stratify = label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iu7qvww4rxg0"
   },
   "outputs": [],
   "source": [
    "from numpy import save \n",
    "# save('X_train_val.npy', X_train_val)\n",
    "# save('y_train_val.npy', y_train_val)\n",
    "# save('X_test.npy', X_test)\n",
    "# save('y_test.npy',y_test)\n",
    "train_val_X = np.load('X_train_val.npy')\n",
    "train_val_y = np.load('y_train_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YB_90Hcsr1PS"
   },
   "outputs": [],
   "source": [
    "#Train/Val split\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_val_X, train_val_y, test_size = 0.11 , shuffle = True, \n",
    "                                                      random_state = 42, stratify = train_val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCDzNPdcr3EP"
   },
   "outputs": [],
   "source": [
    "from numpy import save \n",
    "save('X_train.npy', X_train)\n",
    "save('y_train.npy', y_train)\n",
    "save('X_val.npy', X_val)\n",
    "save('y_val.npy',y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZ0OnqB6CJWt"
   },
   "source": [
    "# Binary Split (But One-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YBFLxxO_CMB5"
   },
   "outputs": [],
   "source": [
    "# HUMAN [1,0]\n",
    "# NONHUMAN [0,1]\n",
    "def process_data(line):\n",
    "    label, feature = line.iloc[:,0],line.iloc[:,1:]\n",
    "    label = label.replace(['BICYCLE','WALL','PILLAR','CAR'], 'NON_HUMAN')\n",
    "    label = np.where(label =='HUMAN',0, label)\n",
    "    label = np.where(label =='NON_HUMAN',1,label)\n",
    "    # for i in ['BICYCLE','WALL','PILLAR','CAR']:\n",
    "    #     label = np.where(label == i,1,label)\n",
    "    label_en = tf.keras.utils.to_categorical(label, num_classes = 2, dtype ='float16')\n",
    "    feature_norm = preprocessing.normalize(feature)\n",
    "    feature_X = feature_norm.reshape(-1,1,5704)\n",
    "    return label_en, feature_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 871671,
     "status": "ok",
     "timestamp": 1601203581798,
     "user": {
      "displayName": "Christian Richardson Escobia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjdPT7lC12Lnkl07FaCCF-69aiWOLTRNP49Dmh7=s64",
      "userId": "09982762860506727889"
     },
     "user_tz": -120
    },
    "id": "vDaTUaJ-CSLk",
    "outputId": "83f4f15d-54c8-4820-c100-c1a545a5345a"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import time \n",
    "chunksize = 1000\n",
    "label, feature = None, None\n",
    "count = 0\n",
    "\n",
    "for chunk in pd.read_csv(\"Dataset/full_echo.csv\",engine = 'python', chunksize=chunksize, skiprows = 0):\n",
    "    start = time.time()\n",
    "    label_en, feature_X = process_data(chunk)\n",
    "    end = time.time()\n",
    "    count += 1 \n",
    "    print(count,label_en, label_en.shape)\n",
    "    print(end-start)\n",
    "    if label is None and feature is None : \n",
    "        label = label_en\n",
    "        feature = feature_X\n",
    "    else:\n",
    "        label = np.append(label,label_en, axis = 0)\n",
    "        feature = np.append(feature, feature_X, axis = 0)\n",
    "    stop = time.time()\n",
    "    print(stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 1175,
     "status": "ok",
     "timestamp": 1601206966882,
     "user": {
      "displayName": "Christian Richardson Escobia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjdPT7lC12Lnkl07FaCCF-69aiWOLTRNP49Dmh7=s64",
      "userId": "09982762860506727889"
     },
     "user_tz": -120
    },
    "id": "XXhPtPPGChd7",
    "outputId": "370e1929-089e-454b-df14-56bb469bd80c"
   },
   "outputs": [],
   "source": [
    "# count_label = tf.math.reduce_sum(label,0)\n",
    "# counting = label.sum(axis=0)\n",
    "# print(counting)\n",
    "label_count = tf.math.argmax(label,axis=-1)\n",
    "unique, counts = np.unique(label_count, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zf-Lu7W9Cj6p"
   },
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(feature,label, test_size = 0.1, shuffle = True, \n",
    "                                                                random_state = 42, stratify = label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 980,
     "status": "ok",
     "timestamp": 1601206309181,
     "user": {
      "displayName": "Christian Richardson Escobia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjdPT7lC12Lnkl07FaCCF-69aiWOLTRNP49Dmh7=s64",
      "userId": "09982762860506727889"
     },
     "user_tz": -120
    },
    "id": "0mnvhkUrVSAx",
    "outputId": "fc205c71-9e67-4404-dc72-0ae903c6299a"
   },
   "outputs": [],
   "source": [
    "len(X_train_val), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_KOLTkxClwj"
   },
   "outputs": [],
   "source": [
    "from numpy import save \n",
    "\n",
    "# save('X_train_val.npy', X_train_val)\n",
    "# save('y_train_val.npy', y_train_val)\n",
    "# save('X_test.npy', X_test)\n",
    "# save('y_test.npy',y_test)\n",
    "train_val_X = np.load('X_train_val.npy')\n",
    "train_val_y = np.load('y_train_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glrGAlyFCnpJ"
   },
   "outputs": [],
   "source": [
    " X_train, X_val, y_train, y_val = train_test_split(train_val_X, train_val_y, test_size = 0.11 , shuffle = True, \n",
    "                                                      random_state = 42, stratify = train_val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 542,
     "status": "ok",
     "timestamp": 1601207432837,
     "user": {
      "displayName": "Christian Richardson Escobia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjdPT7lC12Lnkl07FaCCF-69aiWOLTRNP49Dmh7=s64",
      "userId": "09982762860506727889"
     },
     "user_tz": -120
    },
    "id": "Vwv7enXvZlLj",
    "outputId": "f86aa6ff-a5a6-4d01-c2f3-bf6145d06240"
   },
   "outputs": [],
   "source": [
    "len(X_train), len(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e77_bq-rCpIH"
   },
   "outputs": [],
   "source": [
    "from numpy import save \n",
    "save('X_train.npy', X_train)\n",
    "save('y_train.npy', y_train)\n",
    "save('X_val.npy', X_val)\n",
    "save('y_val.npy',y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PngehiSJwvpu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "0piIWXwDtqXS",
    "CLLL7rxKtwUw"
   ],
   "name": "Data-PreProcessing.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
